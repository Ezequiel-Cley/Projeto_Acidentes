{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158ec9cf",
   "metadata": {},
   "source": [
    "## Leitura dos arquivos na Staigin\n",
    "Aqui é a etapa de descompactação dos arquivos zip e leitura dos mesmo para o tratamento e criação do Armazenamento Bronze Link de acesso aos dados: https://www.gov.br/prf/pt-br/acesso-a-informacao/dados-abertos/dados-abertos-da-prf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile # Biblioteca para extração de arquivos em zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07117bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------- #\n",
    "# ----- Arquivos de nível acidentes todas as causas encontradas ------- #\n",
    "# --------------------------------------------------------------------- #\n",
    "# Caminho do arquivo com todas as informações e cuasas do acidentes.\n",
    "causas_zip = '/Volumes/workspace/projetoacidentes/03_acidentes_por_causa'\n",
    "# Caminho temporario para armazenar os arquivos descompactados\n",
    "diretorio_Staigin_causas = '/Volumes/workspace/dw_acidentes/00_staigin/Acidentes_Causas'\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# ---------- Arquivos de nível acidentes causa principal -------------- #\n",
    "# --------------------------------------------------------------------- #\n",
    "# Caminho dos arquivos no nível mais macro apenas acidentes\n",
    "acidentes_zip = '/Volumes/workspace/projetoacidentes/01_acidentes'\n",
    "# Caminho temporario para armazenar os arquivos descompactados\n",
    "diretorio_Staigin_acidentes = '/Volumes/workspace/dw_acidentes/00_staigin/Acidentes'\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# ---------- Local para armaazenar os arquivos bronze ----------------- #\n",
    "# --------------------------------------------------------------------- #\n",
    "diretorio_bronze = '/Volumes/workspace/dw_acidentes/01_bronze'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dca632",
   "metadata": {},
   "source": [
    "### Realizado descompactação de todos os arquivos e Leitura dos arquivos Acidentes Causas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (Python) Descompacta o arquivo no diretório temporário\n",
    "# Obtendo os dados dos arquivos para extração do zip\n",
    "lista_arquivos = dbutils.fs.ls(causas_zip) \n",
    "lista_arquivos = [f.path for f in lista_arquivos]\n",
    "lista_arquivos = [f.replace('dbfs:/', '/') for f in lista_arquivos] # Criando uma lista com o nome de todos os arquivos que estão na pasta\n",
    "\n",
    "# Loop para interar sobre cada arquivo\n",
    "for i in lista_arquivos:\n",
    "    #caminho_completo_zip = paths_completos # Configurado o caminho completo ao qual deve ser descompactado \n",
    "    with zipfile.ZipFile(i, 'r') as zip_ref:\n",
    "        zip_ref.extractall(diretorio_Staigin_causas) #Define o local onde o arquivo deve ser alocado após extração\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. (PySpark) Lê os arquivos CSV descompactado\n",
    "df_Acidentes_Causas = spark.read.csv(diretorio_Staigin_causas, sep=';', header=True, inferSchema=True, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbfb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizado os 5 primeiros registros para fins de entender o arquivo\n",
    "display(df_Acidentes_Causas.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvado o Dataframe em tabela bronze\n",
    "df_Acidentes_Causas.write.parquet(\n",
    "    f'{diretorio_bronze}/Acidentes_Causas',\n",
    "    mode=\"overwrite\",         # Aqui defino para sempre sobrescrever o arquivo\n",
    "    compression=\"gzip\"        # Comando para salvar o arquivo com compactação\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab6731",
   "metadata": {},
   "source": [
    "### Realizado descompactação de todos os arquivos e Leitura dos arquivos Acidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (Python) Descompacta o arquivo no diretório temporário\n",
    "# Obtendo os dados dos arquivos para extração do zip\n",
    "lista_arquivos = dbutils.fs.ls(acidentes_zip) \n",
    "lista_arquivos = [f.path for f in lista_arquivos]\n",
    "lista_arquivos = [f.replace('dbfs:/', '/') for f in lista_arquivos] # Criando uma lista com o nome de todos os arquivos que estão na pasta\n",
    "\n",
    "# Loop para interar sobre cada arquivo\n",
    "for i in lista_arquivos:\n",
    "    with zipfile.ZipFile(i, 'r') as zip_ref:\n",
    "        zip_ref.extractall(diretorio_Staigin_acidentes) #Define o local onde o arquivo deve ser alocado após extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac105ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. (PySpark) Lê os arquivos CSV descompactado\n",
    "df_Acidentes = spark.read.csv(diretorio_Staigin_acidentes, sep=';', header=True, inferSchema=True, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f657f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizado os 5 primeiros registros para fins de entender o arquivo\n",
    "display(df_Acidentes.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29959ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvado o Dataframe em tabela bronze\n",
    "df_Acidentes.write.parquet(\n",
    "    f'{diretorio_bronze}/Acidentes',\n",
    "    mode=\"overwrite\",         # Aqui defino para sempre sobrescrever o arquivo\n",
    "    compression=\"gzip\"        # Comando para salvar o arquivo com compactação\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
